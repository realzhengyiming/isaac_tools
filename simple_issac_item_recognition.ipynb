{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba541145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda88def",
   "metadata": {},
   "source": [
    "## 简单的道具图像识别, 然后部署到安装上, 这是一个完整的流程,做出来一个简单的应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fba457ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c113b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e22c2",
   "metadata": {},
   "source": [
    "## 编写一个我自己的custom dataset的模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a64fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读取道具的 爬虫的数据映射\n",
    "import json\n",
    "with open(\"id_mapping_dict.json\") as file:\n",
    "    json_content = json.loads(file.read())\n",
    "\n",
    "classes = {v['new_id']:v[\"zh\"]+\":\"+v[\"desc\"] for i, v in json_content.items() if v.get(\"new_id\")!= None}\n",
    "# classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7452d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=64x64>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ls cus_data/\n",
    "from PIL import Image\n",
    "dir_path = \"cus_data/\"\n",
    "image = Image.open(\"cus_data/0.png\")\n",
    "print(image.size)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396186b",
   "metadata": {},
   "source": [
    "# 增加7个随机的背景, 用来合成带背景的图片识别  \n",
    "这样每个类别的对象就增加到7个这么多了.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c44894bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isinstance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e28bb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 打开背景图像和对象图像\n",
    "def add_image_bg(bg_image_path, image_path, output_path):\n",
    "    background = Image.open(bg_image_path)\n",
    "    background = background.resize((64, 64))\n",
    "    object_image = Image.open(image_path)\n",
    "    background.paste(object_image, (0, 0), mask=object_image)\n",
    "\n",
    "    # 保存叠加后的图像\n",
    "    background.save(output_path, \"PNG\")\n",
    "    return background\n",
    "\n",
    "def plot_images(images:Image, images_per_row = 9, figsize=(20,20)):  # pil 的图片才能plot\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # 获取第一张图片的尺寸，假设所有图片尺寸都相同\n",
    "    image_width, image_height = images[0].size\n",
    "\n",
    "    # 计算总的行数\n",
    "    total_rows = len(images) // images_per_row\n",
    "    total_rows += len(images) % images_per_row\n",
    "\n",
    "    # 创建一个新的空白图片，尺寸为所有图片拼接后的尺寸\n",
    "    canvas_width = images_per_row * image_width\n",
    "    canvas_height = total_rows * image_height\n",
    "    canvas = Image.new('RGB', (canvas_width, canvas_height))\n",
    "\n",
    "    # 逐行逐列粘贴图片\n",
    "    for i, image in enumerate(images):\n",
    "        row = i // images_per_row\n",
    "        col = i % images_per_row\n",
    "\n",
    "        # 计算当前图片在画布上的位置\n",
    "        left = col * image_width\n",
    "        top = row * image_height\n",
    "\n",
    "        # 粘贴图片\n",
    "        canvas.paste(image, (left, top))\n",
    "\n",
    "    # 显示拼接后的图片\n",
    "#     canvas.show()\n",
    "    plt.imshow(canvas)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82961be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 26.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合成完毕\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output_dir = \"new_cus_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "bg_files = [i for i in os.listdir(\"./cus_background/\") if i.find(\".\")!=0]\n",
    "meta_images = [i for i in os.listdir(\"./cus_data/\") if i.find(\".\")!=0]\n",
    "meta_images.sort(key=lambda x: int(x.split(\".png\")[0]))\n",
    "meta_images\n",
    "\n",
    "new_meta_images = []\n",
    "for image in tqdm(meta_images[:20]):\n",
    "    for bg in bg_files:\n",
    "        bg_path = f\"cus_background/{bg}\"\n",
    "        image_path = f\"cus_data/{image}\"\n",
    "\n",
    "        output_name = image_path.split(\"/\")[-1].strip(\"png\") + bg_path.split(\"/\")[-1]\n",
    "        output_path = os.path.join(output_dir, output_name)\n",
    "        result = add_image_bg(bg_path, image_path, output_path)\n",
    "#         plt.imshow(result)\n",
    "#         new_meta_images.append(result)\n",
    "print(\"合成完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aa4e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_images(new_meta_images, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c012d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52f9f1",
   "metadata": {},
   "source": [
    "## 构造一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5436b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# net = Net()\n",
    "# print(f\"device:{device}\")\n",
    "# net.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# # 定义损失函数 和优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a085693d",
   "metadata": {},
   "source": [
    "# 此处 开始 00--------00 直接重新用vgg16做的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f7b6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "\n",
    "class IssacCustomDatasets(Dataset):\n",
    "    def __init__(self, img_sort_files, \n",
    "                 img_dir, transform=None, \n",
    "                target_transform=None):\n",
    "        self.img_labels = img_sort_files\n",
    "        # 自定义标签关系, 此处需要排好序的\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):  # 作用是获得label 和 item 即可\n",
    "        filename = self.img_labels[idx]        \n",
    "        # 因为每个图片有9个, 所以自定义的就重新配置一下 labels index 取元素的操作\n",
    "        img_path = os.path.join(self.img_dir, filename)\n",
    "#         image = read_image(img_path)  # 这里也有一个疑惑, 你这个读取的到底是什么格式的东西啊\n",
    "        image = read_image(img_path, mode=torchvision.io.image.ImageReadMode.RGB)\n",
    "\n",
    "        label = int(filename.split(\".\")[0])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bfb4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_output_dir = \"new_mini_20meta_dataset\"\n",
    "os.makedirs(meta_output_dir, exist_ok=True)\n",
    "\n",
    "from_dir = \"new_cus_data\"\n",
    "need_move_images = os.listdir(\"new_cus_data/\")\n",
    "need_move_images = [i for i in need_move_images if int(i.split(\".\")[0]) <= 19]\n",
    "# need_move_images.sort(key=lambda x: int(x.split(\".\")[:1]))\n",
    "need_move_images.sort(key=lambda x: f\"{int(x.split('.')[0]):03d}\" + f\"{x.split('.')[1]}\")\n",
    "need_move_images\n",
    "\n",
    "import shutil\n",
    "for i in need_move_images:\n",
    "    shutil.copy(os.path.join(from_dir, i), os.path.join(meta_output_dir, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5693cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义读取透明背景png的图片\n",
    "from torchvision.transforms import ToTensor\n",
    "        \n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224)),  # 将图片尺寸调整为224x224 大的话, 训练时间会更长, 那之前是怎么训练的, 麻了.\n",
    "        # 增加噪声, 防止过拟合, 因为我还是需要一些现实的照片才可以更准确一些.\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.1,\n",
    "            contrast=0.1,\n",
    "            saturation=0.1,\n",
    "            hue=0.1),  # 抖动图像的亮度、对比度、饱和度和色相\n",
    "        transforms.Lambda(lambda x: x.float()),\n",
    "#         transforms.Normalize(\n",
    "#             [43.11019, 42.666084, 42.702415],\n",
    "#             [100.52347, 99.96471, 100.45631]\n",
    "#         )  # 对图片数据做正则化\n",
    "    ])\n",
    "\n",
    "batch_size = 6\n",
    "\n",
    "# 需要已经排好序\n",
    "train_dataset = IssacCustomDatasets(need_move_images, img_dir=\"new_mini_20meta_dataset/\",\n",
    "                                    transform=transform)\n",
    "# labels = list(range(len(os.listdir(\"cus_data\"))))\n",
    "# train_dataset = IssacCustomDatasets(labels, img_dir=\"cus_data/\",\n",
    "#                                     transform=transform)\n",
    "\n",
    "## dataloader\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True)\n",
    "# test_loader = train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1210495",
   "metadata": {},
   "source": [
    "### 完整类别的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1577ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdf5b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# from torchvision.utils import make_grid\n",
    "\n",
    "# net = vgg16(num_classes=len(classes))  # 这个倒是完整的\n",
    "# # net = MobileNetV3(num_classes=len(classes))  # 这个倒是完整的\n",
    "# net.to(device)  # 重建一个模型, 初始化一个\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # optimizer = optim.SGD(net.parameters(), lr=0.00001, momentum=0.9)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "# # 换cell 才可以好一点, 不然会出问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb235018",
   "metadata": {},
   "source": [
    "## 小批量测试的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07b809d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77.58928680419922, 58.36479568481445, 51.83070755004883]\n",
      "[46.74263381958008, 38.2352409362793, 34.54035186767578]\n"
     ]
    }
   ],
   "source": [
    "# 获取图片数据的 归一化数值\n",
    "global_mean = []\n",
    "global_std = []\n",
    "\n",
    "for images, labels in train_dataloader:   # dataloader is a DataLoader instance with your dataset\n",
    "    numpy_image = images.numpy()\n",
    "    \n",
    "    # Compute mean and std dev\n",
    "    batch_mean = np.mean(numpy_image, axis=(0,2,3))\n",
    "    batch_std = np.std(numpy_image, axis=(0,2,3))\n",
    "    \n",
    "    global_mean.append(batch_mean)\n",
    "    global_std.append(batch_std)\n",
    "\n",
    "# Final mean and std (use np.mean instead of np.average)\n",
    "global_mean = np.mean(global_mean, axis=0).tolist()\n",
    "global_std = np.mean(global_std, axis=0).tolist()\n",
    "print(global_mean)\n",
    "print(global_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d6c3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义读取透明背景png的图片\n",
    "# 根据新的标准差和平均数, 重新运行dataloader\n",
    "from torchvision.transforms import ToTensor\n",
    "        \n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224)),  # 将图片尺寸调整为224x224 大的话, 训练时间会更长, 那之前是怎么训练的, 麻了.\n",
    "        # 增加噪声, 防止过拟合, 因为我还是需要一些现实的照片才可以更准确一些.\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.1,\n",
    "            contrast=0.1,\n",
    "            saturation=0.1,\n",
    "            hue=0.1),  # 抖动图像的亮度、对比度、饱和度和色相\n",
    "        transforms.Lambda(lambda x: x.float()),\n",
    "        transforms.Normalize(\n",
    "            global_mean,\n",
    "            global_std\n",
    "        )  # 对图片数据做正则化\n",
    "    ])\n",
    "\n",
    "\n",
    "# 需要已经排好序\n",
    "train_dataset = IssacCustomDatasets(need_move_images, img_dir=\"new_mini_20meta_dataset/\",\n",
    "                                    transform=transform)\n",
    "# labels = list(range(len(os.listdir(\"cus_data\"))))\n",
    "# train_dataset = IssacCustomDatasets(labels, img_dir=\"cus_data/\", transform=transform)\n",
    "\n",
    "## dataloader\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True)\n",
    "# test_loader = train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9290c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 小批量测试\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "net = vgg16(num_classes=20)  # 这个倒是完整的\n",
    "# net = MobileNetV3(num_classes=20)  # 这个倒是完整的\n",
    "net.to(device)  # 重建一个模型, 初始化一个  或者我直接用 64 不用于训练模型\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.00001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)\n",
    "# 换cell 才可以好一点, 不然会出问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c5da641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## 检查一下transform的抖动效果是怎么样的, \n",
    "# ## 怀疑这个造成问题了\n",
    "\n",
    "# # plt.figure(figsize=(5, 5))、\n",
    "# import numpy as np\n",
    "# check_images = []\n",
    "# for i in range(9):\n",
    "#     plt.figure(figsize=(1, 1))  # 设置图片尺寸为10x8英寸\n",
    "#     path = f\"new_cus_data/1.bg_{i+1}.png\"\n",
    "#     c_image = read_image(path, \n",
    "#                         torchvision.io.image.ImageReadMode.RGB)\n",
    "#     c_image = transform(c_image)\n",
    "#     npimg = c_image.numpy()\n",
    "#     #     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     c_image =  np.transpose(npimg, (1, 2, 0))\n",
    "#     check_images.append(c_image)\n",
    "    \n",
    "    \n",
    "# # plot_images(check_images)\n",
    "#     plt.imshow(c_image)\n",
    "#     plt.title(c_image.shape)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "064d1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 我的目标是解决这个任务, 而不是 玩 无尽的循环游戏,并且没能获得快乐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d361550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af6535e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                      | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "epoch:0, loss: 0.348: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [01:07<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始验证....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save:  20240126__vgg16_0_72.22222222222221.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:1, loss: 0.184:  13%|█████████████▊                                                                                          | 4/30 [00:11<01:13,  2.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:21\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/adam.py:345\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    344\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 345\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    348\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 训练模型\n",
    "\n",
    "# 60 + 60\n",
    "check_iter = 4 # train check batch size \n",
    "train_epoch = 20\n",
    "prefix = \"20240126_\"\n",
    "\n",
    "for epoch in range(train_epoch):\n",
    "    net.train()  # 每个epoch 后切换训练模式, 那么会不会保留之前的训练权重呢?\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_dataloader, 0), total=len(train_dataloader))\n",
    "    for i, data in progress_bar:\n",
    "        inputs, labels = data  # 必须要float 归一化? 浮点类型.\n",
    "        inputs, labels = inputs.float().to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)  # 这一步, 运行有问题, 这是为什么呢, 检查一下图片格式\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(\"per batch loss:\", loss / batch_size)\n",
    "        if i % check_iter == 0:    # 每 4 个小批量打印一次损失值\n",
    "#             print('[epoch: %d, iter_num: %5d] loss: %.3f' % (epoch+1, i + 1, loss / batch_size) )\n",
    "            progress_bar.set_description(f'epoch:{epoch}, loss: {loss / batch_size:.3f}')\n",
    "    if epoch % 1 == 0:  # 每两个epoch进行一次验证\n",
    "        print(\"开始验证....\")\n",
    "        net.eval()\n",
    "        correct = 0  # 记录正确预测的数量\n",
    "        total = 0  # 总的样本数\n",
    "        with torch.no_grad():\n",
    "            progress_bar2 = tqdm(enumerate(train_dataloader, 0), total=int(len(train_dataloader) * 0.2))\n",
    "            for i, data in progress_bar2:\n",
    "                if i >= int(len(train_dataloader) * 0.2):\n",
    "                    break\n",
    "#             for i, data in enumerate(train_dataloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.float().to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # 统计准确率\n",
    "                total += labels.size(0)  # 实际的样本数\n",
    "                correct += (predicted == labels).sum().item()  # 正确预测的样本数\n",
    "\n",
    "#                 print('Real: ', labels, ', Predicted: ', predicted)\n",
    "\n",
    "        accuracy = correct / total * 100  # 计算准确率\n",
    "        progress_bar2.set_description(f'epoch:{epoch}, Accuracy: {accuracy:.3f}')\n",
    "\n",
    "#     print()\n",
    "    if epoch % 5 == 0:\n",
    "        save_model_path = f\"{prefix}_vgg16_{epoch}_{accuracy}.pth\"\n",
    "        torch.save(net.state_dict(), save_model_path)\n",
    "        print(f\"model save: \", save_model_path)\n",
    "    torch.cuda.empty_cache()\n",
    "print('Finished Training')\n",
    "\n",
    "save_model_path = f\"{prefix}_vgg16_{epoch}_{accuracy}.pth\"\n",
    "torch.save(net.state_dict(), save_model_path)\n",
    "print(f\"model save: \", save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "045f609e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(train_dataloader) * 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6066be",
   "metadata": {},
   "source": [
    "model save:  20240126__vgg16_19_100.0.pth  \n",
    "CPU times: user 22min 21s, sys: 3min 49s, total: 26min 11s  \n",
    "Wall time: 49min 32s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8121dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf24788",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 大概40个epoch就可以了?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da3dbc0",
   "metadata": {},
   "source": [
    "## 用本地的图片进行测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c37c06",
   "metadata": {},
   "source": [
    "上面我保存到mps 中执行的保存, 所以后面也需要使用mps才可以? 或者转换成onnx 统一的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f917c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {v['new_id']:v for i, v in json_content.items() if v.get(\"new_id\")!=None}\n",
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cd9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = vgg16(num_classes=20).to(device)\n",
    "# model.load_state_dict(torch.load(save_model_path))\n",
    "# model.load_state_dict(torch.load(\"20240125_13:41__vgg16_0.pth\"))\n",
    "model.load_state_dict(torch.load(\"20240126__vgg16_10_97.22222222222221.pth\"))  # 不用百分百的\n",
    "\n",
    "\n",
    "def eval_predict(model, image_path):\n",
    "#     test_img = os.path.join(\"cus_test_data\", test_img)\n",
    "    image = read_image(image_path, mode=torchvision.io.image.ImageReadMode.RGB)\n",
    "    # image = image.resize(64, 64)\n",
    "    plt.imshow(np.transpose(image, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    model.eval()\n",
    "    timg = transform(image)\n",
    "    timg = timg.to(device)\n",
    "    timg1 = timg.unsqueeze(0)\n",
    "    result = model(timg1)\n",
    "    result\n",
    "    _, predicted = torch.max(result, 1)\n",
    "    print(predicted.item())\n",
    "\n",
    "    print(predicted.item(), \n",
    "          new_dict[predicted.item()]['zh'], \n",
    "          new_dict[predicted.item()]['en'],\n",
    "          new_dict[predicted.item()]['desc'])\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(predicted[0], dim=0)\n",
    "    top_k_values, top_k_indices = torch.topk(probs, 20)\n",
    "    print(\"Top 20 categories are: \", top_k_indices)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85757df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 打开一个图片, 加一个预处理, 检查一下\n",
    "# import cv2 \n",
    "\n",
    "# test_img = \"cus_data/2.png\"\n",
    "# print(\"real label: \", test_img.split(\"/\")[-1])\n",
    "# # image = read_image()\n",
    "# # test_img = \"/Users/zhengyiming/Downloads/iShot_2024-01-24_16.17.44.png\"\n",
    "\n",
    "\n",
    "\n",
    "# for test_img in os.listdir(\"cus_test_data/\"):\n",
    "#     print(test_img)\n",
    "#     if str(test_img).split(\".\")[-1] not in [\"png\", \"jpeg\", \"jpg\"]:\n",
    "#         continue\n",
    "# #     test_img = \"cus_test_data/iShot_2024-01-25_14.09.48.png\"\n",
    "#     image_path = os.path.join(\"cus_test_data\", test_img)\n",
    "#     eval_predict(model, image_path)\n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ef69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"new_cus_data/1.bg_3.png\"\n",
    "image_path = \"/Users/zhengyiming/PycharmProjects/Isaac/new_cus_data/2.bg_8.png\"\n",
    "\n",
    "image_path = \"/Users/zhengyiming/Downloads/iShot_2024-01-26_12.22.24.png\"\n",
    "\n",
    "# image_path = \"/Users/zhengyiming/Downloads/iShot_2024-01-26_12.16.24.png\"\n",
    "# image_path = \"/Users/zhengyiming/Downloads/iShot_2024-01-26_12.16.01.png\"\n",
    "# image_path = \"/Users/zhengyiming/Downloads/iShot_2024-01-26_12.17.48.png\"\n",
    "# image_path = \"/Users/zhengyiming/Downloads/iShot_2024-01-26_12.16.20.png\"\n",
    "\n",
    "# for i in os.listdir(\"new_cus_data/\"):\n",
    "#     if i.split(\".\")[-1] not in [\"png\", \"jpeg\", \"jpg\"]:\n",
    "#         continue\n",
    "        \n",
    "# image_path = os.path.join(\"new_cus_data\", i)\n",
    "\n",
    "print(\"real label: \", image_path.split(\"/\")[-1])    \n",
    "# image_path = os.path.join(\"cus_test_data\", test_img)\n",
    "eval_predict(model, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6cd860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e88768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "# probabilities_percent = probabilities * 100\n",
    "# for i in range(probabilities_percent.shape[0]):\n",
    "#     print(f\"Image {i+1}:\")\n",
    "#     for j in range(probabilities_percent.shape[1]):\n",
    "#         print(f\"    Class {j}: {probabilities_percent[i][j].item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed98c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdaca03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19981c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40f036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ecbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78930b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74131998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img.split(\".\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "# probabilities_percent = probabilities * 100\n",
    "# for i in range(probabilities_percent.shape[0]):\n",
    "#     print(f\"Image {i+1}:\")\n",
    "#     for j in range(probabilities_percent.shape[1]):\n",
    "#         print(f\"    Class {j}: {probabilities_percent[i][j].item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454cb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes)\n",
    "\n",
    "## 明天把这个模型简单的封装一下, 增加一个前端, 摄像头框选的操作. 试试怎么搞\n",
    "# 封装起来直接可以用了, 有意思."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save my net model\n",
    "# print(epoch)\n",
    "# save_model_path = f\"net_{epoch}.pth\"\n",
    "# torch.save(net.state_dict(), save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00eed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecdbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = vgg16(num_classes=len(classes)).to(device)\n",
    "# model.load_state_dict(torch.load(save_model_path))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3844be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3772a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 检查模型, 说明模型根本没训练到\n",
    "# import torch\n",
    "# # 在测试集上进行推理验证\n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for data in train_dataloader:\n",
    "#         images, labels = data\n",
    "# #         images = images.float()\n",
    "\n",
    "#         ## 并排显示两个图片, 完全没训练到, 这是, loss\n",
    "#         print(len(labels))\n",
    "        \n",
    "#         sub_images = [images[i].numpy().transpose((1, 2, 0))\n",
    "#                      for i in range(batch_size)]\n",
    "        \n",
    "# #         img1 = images[0].numpy().transpose((1, 2, 0))\n",
    "# #         img2 = images[1].numpy().transpose((1, 2, 0))\n",
    "# #         combined_img = np.concatenate((img1, img2), axis=1)\n",
    "\n",
    "#         combined_img = np.concatenate(sub_images, axis=1)\n",
    "#         plt.imshow(combined_img)\n",
    "        \n",
    "#         images = images.float()\n",
    "\n",
    "        \n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "#         sub_labels = [new_dict[i]['en'] for i in predicted.tolist()]\n",
    "#         plt.title(sub_labels)\n",
    "#         plt.show()  # 归一化, 输入前. 这个倒也是没问题. \n",
    "\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "# accuracy = 100 * correct / total\n",
    "# print('Accuracy of the network on the test images: %d %%' % accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## vgg 16就足够了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 接下来的路线\n",
    "\n",
    "## 1. 增加训练数据量, 增加数据增强\n",
    "## 2. 测试和验证简单的 自定义模型\n",
    "\n",
    "## 3. 继续接下来的部署流程, \n",
    "### 3.1 部署到移动设备, 安卓或者 ios\n",
    "### 3.2 直接部署到安卓本身  \n",
    "\n",
    "## 4. 图像识别改成 目标检测!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de8034f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820b2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

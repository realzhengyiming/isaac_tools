{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba541145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda88def",
   "metadata": {},
   "source": [
    "## 简单的道具图像识别, 然后部署到安装上, 这是一个完整的流程,做出来一个简单的应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fba457ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c113b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e22c2",
   "metadata": {},
   "source": [
    "## 编写一个我自己的custom dataset的模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a64fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读取道具的 爬虫的数据映射\n",
    "import json\n",
    "with open(\"id_mapping_dict.json\") as file:\n",
    "    json_content = json.loads(file.read())\n",
    "\n",
    "classes = {v['new_id']:v[\"zh\"]+\":\"+v[\"desc\"] for i, v in json_content.items() if v.get(\"new_id\")!= None}\n",
    "# classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7452d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAALqElEQVR4nO2aa2wc13XHf3deuzO7q10+JPElilqKEteiID9kR3JiJ01tp02RIPnkNGiBImhhFAGaIm6DpojzwQ5qoGiMokCbAgX8wQiQuOiHtEUCuAaSNLJCyXLs6EmJkkiKIvUiRS65s7vzunP7YXZtWbZRxJxFDZQ/YoDFLOfOmbPn/s+dcy5ssskmm2yyyf9bRIfGtSwo65C/86QEN4AZIOjQfX9jjE4MasNWBd9QsF+1zglAg9M2PNOExU7c98OQtgMsC8oxHI7hCQ0G218oIIZ+DY5YMPlRi4RUsGEwCy9mYUoHXwN156GDn4WpLLxo3+Gc/0vSioCcBcMxHFLwRMbSBwvdWYysgYqTSSA0QeRFlrfijfuBLIgkEo4FMA/UU7LjNyYVEcxDJYRvAg9KKHf15ayxx3fSPVJEhjEAuqmxMrfGxVevsHqjHujJFDhhwvMuTKVhx4chlQiIoCTgEwp2KcCwDXpGS/Tt6yVoRKDAyOoopdBMHQmWDuOAiMF5nyHflUU6mT06kgXaYaUUyDBGyRgERL5E+hGKRBQFuDq4d19/ZxaBzmaPVBxgQDWE1wBPwHDYjHLLl6touoaVtzCzBjKUGBmd7nIJqQn8FY8wkE4ID1lAABcBLBiL4eE7s0gns0daC6GcBcMaHJTwTdPSK5nuLFtHiow9NkzXSJE4VIReROAG3Jpe4VKiBXUd5gUc8eBpgCx8F3g0hLICq2VkYCYPPilSjoS0pkA9gKkCSAVrUSBp3Kijx4rIH0Q3dGQQYtoGhe0OkR+hJ1qQ06EC2DY8LoEInsjoYqTQa2M6JgqImp3LHqlqgAQb0CEJLSNjYGR0dEtDeAIVK2QQE/mSqKUFcXJBv4JnYyCC/nzeYvdjO+nd0wXA7UtVLr56heaN+lYdviHgRD6l7NExEdSAoBmyPLOGbuhkihambSKjd7Qg0gTBikcUyIyCCUMX5Hptto92sa3SQ+/ebjRdgILZ1xZpTYlxAZkISmnYqqUxSBsdmoBsC8v6cpPT/3mZMz+6SH2pgW5oxJHC6XGo/F6ZyuM7yfXYhEAIZPMWlcd2UvlsmeyWDEEtRMnWQipNQ+8grQjIm1AO4GAMpWxGp9hrIyydZiMilgqlQClFLON3tCCQ6KaGbmg4PVn6dnfRP9FL964SQT3ErwWsX6uxfLlK2IwQifrPqGQBVU3D8FQckINdETyrEgf02j02458t0zVSpLkeIAQ4PTZRIFGxQmkKGUgiLyLyI5y8SeW3dzIwsRWnx0ZJheWYrF93Of+TWW7NVAlWPDRYAv5WwTE3EcEN0xENgGShYzkmhb4cQheEzQgZShQKEUMsVXJECsvS6S2XKPQ7LF+uEjQjMo7JykyVm2eXqa75dRPmdTgm4L8+cmmwDrMmfFuDgxr8Zf12c/zMT2ZYurBC5XfLlEa2IH1JpBQyloDCUK1bi+QQmmDtmsuZf59medEll8tAIAndACN5+OdjOBEkUZAaaUWAG8KpItQ8+OPQl9QXXYJVj1yfg++FWAWDrJOhkNmCpgl85RFFIVEYocWCwPNxMg6lYonmTcXK1VUkiezrUM3Aa2swm5K9b5NqFrgbtxFx8j+meetfz7J0dZmMk2F0eJRdO0YxDINGs47fCGnWAtbX1in2F/j8136Hzz31BN1dBQBkJw0k5XqA38oCTtaiv7+EaVs0myE9xQLlgTHGh8cZHdwDCjShE40Kmg8kaW5sZJx9uyeYGN/PzYElFqYWOPnGNO6NKoEXlnz4hAXZtOsHqdYDYjgYwfDOXdtyj/3Rb7Fj3yANt0l3oZv9E/cysG2ATCYLCnzfY/HWAr8+9SYA9+6/n6H+neRzeW6vL/Gr08eZ/Onr/OylX7Awt1w3YF6DN9KuH6QSATE4Ah5SsFcBWcfmgY/fz32PHKBer+OYeYbyZfIUCAmBJEN0dfXg9GRRwGjvXkr0AOB0O9iftJC+5Pi/nUBBTkFFgPYB9YMPTSoO0MGNwY3aJyIY2TrGvdaD1K06IREKQZMGiiTkJREaOkOlkVZtQMPHI0uWEiV66Gah7wZIrT0kxgfUDzbCRh2Qs2DYg4OAsyWfY6i8g3vu2c3S7DJvytN4wsO0Dfp2bCfn5FCtv5gYgSBvFBEIFDHV+iqL84uEDUlO5FhaWGH/ffvRMzYLM1dpunVHJvUDLS0t2JAGtOd+lIjf8IEDldzXv/M0uZzNy//yMpcvXSUmZrxS5qm/eooDlQN4eES8HSsINAwMMlicnDrJ957/HuenZjHQGR3bwZN/8iT1epMXvvVdTp6cqmswb6SoBRuKgHYtULRqgbktDpUHx4iiiHOnznDm7AwA7u1lvlz9MiYmASE6Cq2VgduRYGJSq9Z48+gbXJxJFnp+c42BPV/DMAxyWxwU5ESiBdm03gZT0YB2GDV8l+lr54iiEC+svf29pikQydzX0NAwsZJiDwHBOyMIlfxvCy+sMX3tLIZh0vDdd90rLTbkgLtrgdXVtdwvfzZJNuswMLIPzXKRUjG+bxe5QokQnyYut1dXuD5zA4D+ch89Xd0YCHKFEuP3PkxkzqLrgr6BPKdOnMPzGlRX1xDJnJ9XyRSobvzxN+7Qd9UCM7ZVKQ52Ux7ZyyOf+QoDu+6h7kb09gg+9vEuSl0RN6Mb/PKnx/j+sy8D8AfffpKHP32I7UYf1VWD40dXWb6tyOUNrs2e48grLzIzd4G1xRX8ZjDVeid446NSEmvXAmOg0WwGuJduYPpbKfzhAcbuP4D0oJiHYnGFqjvF5OQkx3/+Og23AcDxnx9HCMWnDn+KYrHC+IE9rLmgZ6FWN7l6YYXFqzfQARMaJrxegwsbffA2qWiATBoYeS35jNRMZq5LClehNwulfICjZZieXealZ3+Iu+7yZ3/95wD8w9/8PdNHZqj80wT9+zMIAtZrFstLMHNdIjUTePulJS/varlvlFQcoEFDwuskTdCy77nW+VPHELrN+NgQhS0mq36VZlQnZ+XZPtzPo48/CsCPv/9j3EaNZlRn1a+yWu/h2vUa5y8ucP7UMXzPRbujEqRBIw2b26TaFxBwKIbnTMsZtLqG2Ln7MIe+8C32fWwnQ0NHyFvXCG9aFK1eyvvKAMycnWEtWMbcHuAGAywsPMLZ41c49qPvcOXSJMHqAmHQWNTgGdWBZmqqfQEb1gU8EgSNw42b02XRcK3S0CGa4QiXuo+yd1Tnk49+hn5rB2tqDYDKxB6uBzb//YtXuHD5AtWVgCsn55g/9SrV2rXAgBkjaYikWglqk3Zatayko3NYwXMZ3RzM9ezCyNsIo8o9E/38/l98ifEH9tJwkx/Ryec4/6sL/ODvfsi5M9dRUYnIbVK/PYsvw0UBz4gObqhIuyYYBHDehnUFhyIZHlq+NV2ObyXCFa2usmPiKNdvLhI0k2exbIvLb81x6sg5bi6tA1fQWq2wTv7ybTpVbtds6CPZL/Ccgv0xkDV0eoa6sfPZd22caLoetxdW8CKJDihoz/mOb6XpVFVYKDAEWCLZCwBAGEnm596/pmkBJvgCpiVMavBKE651yL7OYsNQBn6QhatZUHbryIKyQJmgjNZhts7ZoDIwZ8MXnSR6OlqvbJN2BBScpL+/X8A2AQ0Fv46TzlcEqFbfsEhrhwhwUUGt9Qo0B0QyqfpoJCvMjpKqBjhwv4QXRFLC+kcBFwREEYQCGiJxAgIeAv4Z0BU8rcGbAAr2KPiqSET0q34y/ztKpzRgDXjNg0vtEzYMSihAsnJUycrRJVH5OYAMVAV8BdgGfNqBUiPZOVJ7zx1SIu0sUHBgDFCNpFrjATjQJ+EFDe4DkDCnwUsSTofJA/qt67MOVCQ8IOBPFdR0+HqjFSGdIO0IqN1l7J2asJ13trzUgaPhexucXgPeysA68KUO2PceOrUOAN5XE6YBJNTC5LP/AZdmnWTrjOj0FOi4h1u8RxP+F7wGvNVJg9p0NAL4AE3YZJNNNtlkk0022eQjwP8AODwJQ+QwY2UAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=64x64>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ls cus_data/\n",
    "from PIL import Image\n",
    "dir_path = \"cus_data/\"\n",
    "image = Image.open(\"cus_data/0.png\")\n",
    "print(image.size)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cf6242",
   "metadata": {},
   "source": [
    "# 增加7个随机的背景, 用来合成带背景的图片识别  \n",
    "这样每个类别的对象就增加到7个这么多了.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dc86528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isinstance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f69f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 打开背景图像和对象图像\n",
    "def add_image_bg(bg_image_path, image_path, output_path):\n",
    "    background = Image.open(bg_image_path)\n",
    "    background = background.resize((64, 64))\n",
    "    object_image = Image.open(image_path)\n",
    "    background.paste(object_image, (0, 0), mask=object_image)\n",
    "\n",
    "    # 保存叠加后的图像\n",
    "    background.save(output_path, \"PNG\")\n",
    "    return background\n",
    "\n",
    "def plot_images(images:Image, images_per_row = 9, figsize=(20,20)):  # pil 的图片才能plot\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # 获取第一张图片的尺寸，假设所有图片尺寸都相同\n",
    "    image_width, image_height = images[0].size\n",
    "\n",
    "    # 计算总的行数\n",
    "    total_rows = len(images) // images_per_row\n",
    "    total_rows += len(images) % images_per_row\n",
    "\n",
    "    # 创建一个新的空白图片，尺寸为所有图片拼接后的尺寸\n",
    "    canvas_width = images_per_row * image_width\n",
    "    canvas_height = total_rows * image_height\n",
    "    canvas = Image.new('RGB', (canvas_width, canvas_height))\n",
    "\n",
    "    # 逐行逐列粘贴图片\n",
    "    for i, image in enumerate(images):\n",
    "        row = i // images_per_row\n",
    "        col = i % images_per_row\n",
    "\n",
    "        # 计算当前图片在画布上的位置\n",
    "        left = col * image_width\n",
    "        top = row * image_height\n",
    "\n",
    "        # 粘贴图片\n",
    "        canvas.paste(image, (left, top))\n",
    "\n",
    "    # 显示拼接后的图片\n",
    "#     canvas.show()\n",
    "    plt.imshow(canvas)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c73fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 23.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合成完毕\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output_dir = \"new_cus_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "bg_files = [i for i in os.listdir(\"./cus_background/\") if i.find(\".\")!=0]\n",
    "meta_images = [i for i in os.listdir(\"./cus_data/\") if i.find(\".\")!=0]\n",
    "meta_images.sort(key=lambda x: int(x.split(\".png\")[0]))\n",
    "meta_images\n",
    "\n",
    "new_meta_images = []\n",
    "for image in tqdm(meta_images[:20]):\n",
    "    for bg in bg_files:\n",
    "        bg_path = f\"cus_background/{bg}\"\n",
    "        image_path = f\"cus_data/{image}\"\n",
    "\n",
    "        output_name = image_path.split(\"/\")[-1].strip(\"png\") + bg_path.split(\"/\")[-1]\n",
    "        output_path = os.path.join(output_dir, output_name)\n",
    "        result = add_image_bg(bg_path, image_path, output_path)\n",
    "#         plt.imshow(result)\n",
    "#         new_meta_images.append(result)\n",
    "print(\"合成完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5065ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_images(new_meta_images, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c012d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52f9f1",
   "metadata": {},
   "source": [
    "## 构造一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5436b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# net = Net()\n",
    "# print(f\"device:{device}\")\n",
    "# net.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# # 定义损失函数 和优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a085693d",
   "metadata": {},
   "source": [
    "# 此处 开始 00--------00 直接重新用vgg16做的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f7b6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "\n",
    "class IssacCustomDatasets(Dataset):\n",
    "    def __init__(self, img_sort_files, \n",
    "                 img_dir, transform=None, \n",
    "                target_transform=None):\n",
    "        self.img_labels = img_sort_files\n",
    "        # 自定义标签关系, 此处需要排好序的\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):  # 作用是获得label 和 item 即可\n",
    "        filename = self.img_labels[idx]        \n",
    "        # 因为每个图片有9个, 所以自定义的就重新配置一下 labels index 取元素的操作\n",
    "        img_path = os.path.join(self.img_dir, filename)\n",
    "#         image = read_image(img_path)  # 这里也有一个疑惑, 你这个读取的到底是什么格式的东西啊\n",
    "        image = read_image(img_path, mode=torchvision.io.image.ImageReadMode.RGB)\n",
    "\n",
    "        label = int(filename.split(\".\")[0])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bfb4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_output_dir = \"new_mini_20meta_dataset\"\n",
    "os.makedirs(meta_output_dir, exist_ok=True)\n",
    "\n",
    "from_dir = \"new_cus_data\"\n",
    "need_move_images = os.listdir(\"new_cus_data/\")\n",
    "need_move_images = [i for i in need_move_images if int(i.split(\".\")[0]) <= 19]\n",
    "# need_move_images.sort(key=lambda x: int(x.split(\".\")[:1]))\n",
    "need_move_images.sort(key=lambda x: f\"{int(x.split('.')[0]):03d}\" + f\"{x.split('.')[1]}\")\n",
    "need_move_images\n",
    "\n",
    "import shutil\n",
    "for i in need_move_images:\n",
    "    shutil.copy(os.path.join(from_dir, i), os.path.join(meta_output_dir, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5693cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义读取透明背景png的图片\n",
    "from torchvision.transforms import ToTensor\n",
    "        \n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.ToTensor(),\n",
    "        transforms.Resize((64, 64)),  # 将图片尺寸调整为224x224 大的话, 训练时间会更长, 那之前是怎么训练的, 麻了.\n",
    "        # 增加噪声, 防止过拟合, 因为我还是需要一些现实的照片才可以更准确一些.\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.1,\n",
    "            contrast=0.1,\n",
    "            saturation=0.1,\n",
    "            hue=0.1),  # 抖动图像的亮度、对比度、饱和度和色相\n",
    "        transforms.Lambda(lambda x: x.float()),\n",
    "#         transforms.Normalize(\n",
    "#             [43.11019, 42.666084, 42.702415],\n",
    "#             [100.52347, 99.96471, 100.45631]\n",
    "#         )  # 对图片数据做正则化\n",
    "    ])\n",
    "\n",
    "batch_size = 16 # 6 244 的版本, 4倍, 那我可以加到18\n",
    "\n",
    "# 需要已经排好序\n",
    "train_dataset = IssacCustomDatasets(need_move_images, img_dir=\"new_mini_20meta_dataset/\",\n",
    "                                    transform=transform)\n",
    "# labels = list(range(len(os.listdir(\"cus_data\"))))\n",
    "# train_dataset = IssacCustomDatasets(labels, img_dir=\"cus_data/\",\n",
    "#                                     transform=transform)\n",
    "\n",
    "## dataloader\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True)\n",
    "# test_loader = train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1210495",
   "metadata": {},
   "source": [
    "### 完整类别的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1577ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdf5b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# from torchvision.utils import make_grid\n",
    "\n",
    "# net = vgg16(num_classes=len(classes))  # 这个倒是完整的\n",
    "# # net = MobileNetV3(num_classes=len(classes))  # 这个倒是完整的\n",
    "# net.to(device)  # 重建一个模型, 初始化一个\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # optimizer = optim.SGD(net.parameters(), lr=0.00001, momentum=0.9)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "# # 换cell 才可以好一点, 不然会出问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb235018",
   "metadata": {},
   "source": [
    "## 小批量测试的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07b809d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75.5831527709961, 57.4347038269043, 50.27412033081055]\n",
      "[48.67875289916992, 41.2623405456543, 36.199893951416016]\n"
     ]
    }
   ],
   "source": [
    "# 获取图片数据的 归一化数值\n",
    "global_mean = []\n",
    "global_std = []\n",
    "\n",
    "for images, labels in train_dataloader:   # dataloader is a DataLoader instance with your dataset\n",
    "    numpy_image = images.numpy()\n",
    "    \n",
    "    # Compute mean and std dev\n",
    "    batch_mean = np.mean(numpy_image, axis=(0,2,3))\n",
    "    batch_std = np.std(numpy_image, axis=(0,2,3))\n",
    "    \n",
    "    global_mean.append(batch_mean)\n",
    "    global_std.append(batch_std)\n",
    "\n",
    "# Final mean and std (use np.mean instead of np.average)\n",
    "global_mean = np.mean(global_mean, axis=0).tolist()\n",
    "global_std = np.mean(global_std, axis=0).tolist()\n",
    "print(global_mean)\n",
    "print(global_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ceec3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义读取透明背景png的图片\n",
    "# 根据新的标准差和平均数, 重新运行dataloader\n",
    "from torchvision.transforms import ToTensor\n",
    "        \n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.ToTensor(),\n",
    "        transforms.Resize((64, 64)),  # 将图片尺寸调整为224x224 大的话, 训练时间会更长, 那之前是怎么训练的, 麻了.\n",
    "        # 增加噪声, 防止过拟合, 因为我还是需要一些现实的照片才可以更准确一些.\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.1,\n",
    "            contrast=0.1,\n",
    "            saturation=0.1,\n",
    "            hue=0.1),  # 抖动图像的亮度、对比度、饱和度和色相\n",
    "        transforms.Lambda(lambda x: x.float()),\n",
    "        transforms.Normalize(\n",
    "            global_mean,\n",
    "            global_std\n",
    "        )  # 对图片数据做正则化\n",
    "    ])\n",
    "\n",
    "\n",
    "# 需要已经排好序\n",
    "train_dataset = IssacCustomDatasets(need_move_images, img_dir=\"new_mini_20meta_dataset/\",\n",
    "                                    transform=transform)\n",
    "# labels = list(range(len(os.listdir(\"cus_data\"))))\n",
    "# train_dataset = IssacCustomDatasets(labels, img_dir=\"cus_data/\", transform=transform)\n",
    "\n",
    "## dataloader\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True)\n",
    "# test_loader = train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9290c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## 小批量测试\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "# 为了让原本的vgg16 支持64*64的图片输入, 全连接层调整, 原来7*7的特征图改成2*2就可以了\n",
    "class VGG16_S(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG16_S, self).__init__()\n",
    "        model = vgg16(pretrained=False)  # 控制变量法, 晚点再看看 True的效果如何\n",
    "        self.features = model.features  # 只取了feature\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 4096),  # 修改此处的第一个参数\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "      \n",
    "net = VGG16_S(num_classes=20).to(device)  # 将模型送入设备\n",
    "\n",
    "# net = vgg16(num_classes=20)  # 这个倒是完整的\n",
    "# net = MobileNetV3(num_classes=20)  # 这个倒是完整的\n",
    "# net.to(device)  # 重建一个模型, 初始化一个  或者我直接用 64 不用于训练模型\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.00001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)\n",
    "# 换cell 才可以好一点, 不然会出问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c5da641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## 检查一下transform的抖动效果是怎么样的, \n",
    "# ## 怀疑这个造成问题了\n",
    "\n",
    "# # plt.figure(figsize=(5, 5))、\n",
    "# import numpy as np\n",
    "# check_images = []\n",
    "# for i in range(9):\n",
    "#     plt.figure(figsize=(1, 1))  # 设置图片尺寸为10x8英寸\n",
    "#     path = f\"new_cus_data/1.bg_{i+1}.png\"\n",
    "#     c_image = read_image(path, \n",
    "#                         torchvision.io.image.ImageReadMode.RGB)\n",
    "#     c_image = transform(c_image)\n",
    "#     npimg = c_image.numpy()\n",
    "#     #     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     c_image =  np.transpose(npimg, (1, 2, 0))\n",
    "#     check_images.append(c_image)\n",
    "    \n",
    "    \n",
    "# # plot_images(check_images)\n",
    "#     plt.imshow(c_image)\n",
    "#     plt.title(c_image.shape)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d98547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 我的目标是解决这个任务, 而不是 玩 无尽的循环游戏,并且没能获得快乐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acf0fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af6535e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0, loss: 0.191: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:13<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始验证....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, Accuracy: 9.444\n",
      "model save:  20240127_64x64_20class_18batch__vgg16_0_9.444444444444445.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:1, loss: 0.187: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.03s/it]\n",
      "epoch:2, loss: 0.184: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始验证....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2, Accuracy: 5.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:3, loss: 0.186: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.02s/it]\n",
      "epoch:4, loss: 0.183: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始验证....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4, Accuracy: 8.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:5, loss: 0.185: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save:  20240127_64x64_20class_18batch__vgg16_5_8.88888888888889.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:6, loss: 0.181: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始验证....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6, Accuracy: 10.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:7, loss: 0.182: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.01s/it]\n",
      "epoch:8, loss: 0.183: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始验证....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8, Accuracy: 5.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:9, loss: 0.167: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.02s/it]\n",
      "epoch:10, loss: 0.176: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始验证....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10, Accuracy: 12.222\n",
      "model save:  20240127_64x64_20class_18batch__vgg16_10_12.222222222222221.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:11, loss: 0.180: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.02s/it]\n",
      "epoch:12, loss: 0.176: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始验证....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:03<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12, Accuracy: 11.111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:13, loss: 0.176: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.04s/it]\n",
      "epoch:14, loss: 0.158: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始验证....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14, Accuracy: 22.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:15, loss: 0.172: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save:  20240127_64x64_20class_18batch__vgg16_15_22.22222222222222.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:16, loss: 0.158: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始验证....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16, Accuracy: 26.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:17, loss: 0.167: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.01s/it]\n",
      "epoch:18, loss: 0.119: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始验证....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18, Accuracy: 31.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:19, loss: 0.150: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n",
      "model save:  20240127_64x64_20class_18batch__vgg16_19_31.666666666666664.pth\n",
      "CPU times: user 6min 30s, sys: 28.7 s, total: 6min 59s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 训练模型\n",
    "\n",
    "# 60 + 60\n",
    "check_iter = 4 # train check batch size \n",
    "train_epoch = 20\n",
    "model_output_dir = \"20classes\"\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "prefix = \"20240127_64x64_20class_18batch_\"\n",
    "\n",
    "for epoch in range(train_epoch):\n",
    "    net.train()  # 每个epoch 后切换训练模式, 那么会不会保留之前的训练权重呢?\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_dataloader, 0), total=len(train_dataloader))\n",
    "    for i, data in progress_bar:\n",
    "        inputs, labels = data  # 必须要float 归一化? 浮点类型.\n",
    "        inputs, labels = inputs.float().to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)  # 这一步, 运行有问题, 这是为什么呢, 检查一下图片格式\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(\"per batch loss:\", loss / batch_size)\n",
    "        if i % check_iter == 0:    # 每 4 个小批量打印一次损失值\n",
    "#             print('[epoch: %d, iter_num: %5d] loss: %.3f' % (epoch+1, i + 1, loss / batch_size) )\n",
    "            progress_bar.set_description(f'epoch:{epoch}, loss: {loss / batch_size:.3f}')\n",
    "    if epoch % 2 == 0:  # 每两个epoch进行一次验证\n",
    "        print(\"开始验证....\")\n",
    "        net.eval()\n",
    "        correct = 0  # 记录正确预测的数量\n",
    "        total = 0  # 总的样本数\n",
    "        with torch.no_grad():\n",
    "            progress_bar2 = tqdm(enumerate(train_dataloader, 0), total=int(len(train_dataloader)))\n",
    "            for i, data in progress_bar2:\n",
    "                if i >= int(len(train_dataloader)):\n",
    "                    break\n",
    "#             for i, data in enumerate(train_dataloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.float().to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # 统计准确率\n",
    "                total += labels.size(0)  # 实际的样本数\n",
    "                correct += (predicted == labels).sum().item()  # 正确预测的样本数\n",
    "\n",
    "#                 print('Real: ', labels, ', Predicted: ', predicted)\n",
    "\n",
    "        accuracy = correct / total * 100  # 计算准确率\n",
    "        print(f'epoch:{epoch}, Accuracy: {accuracy:.3f}' )\n",
    "#         progress_bar2.set_description(f'epoch:{epoch}, Accuracy: {accuracy:.3f}')\n",
    "\n",
    "#     print()\n",
    "    if epoch % 5 == 0:\n",
    "        save_model_path = f\"{prefix}_vgg16_{epoch}_{accuracy}.pth\"\n",
    "        torch.save(net.state_dict(), os.path.join(model_output_dir, save_model_path))\n",
    "        print(f\"model save: \", save_model_path)\n",
    "    torch.cuda.empty_cache()\n",
    "print('Finished Training')\n",
    "\n",
    "save_model_path = f\"{prefix}_vgg16_{epoch}_{accuracy}.pth\"\n",
    "torch.save(net.state_dict(), os.path.join(model_output_dir, save_model_path))\n",
    "print(f\"model save: \", save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfba71a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4528e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(len(train_dataloader))\n",
    "# 下面那个是batch_size = 2 还是4的, 20epoch, 一个钟,确实久. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33291d23",
   "metadata": {},
   "source": [
    "model save:  20240126__vgg16_19_100.0.pth  \n",
    "CPU times: user 22min 21s, sys: 3min 49s, total: 26min 11s  \n",
    "Wall time: 49min 32s  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef1e06",
   "metadata": {},
   "source": [
    "Finished Training  \n",
    "model save:  20240126__vgg16_19_94.44444444444444.pth  \n",
    "CPU times: user 6min 41s, sys: 30.7 s, total: 7min 12s  \n",
    "Wall time: 4min 47s  \n",
    "batch_size = 16, input 64*64 速度果然快了很多  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a7a9d",
   "metadata": {},
   "source": [
    "Finished Training  \n",
    "model save:  20240127_64x64_20class_18batch__vgg16_19_5.0.pth  \n",
    "CPU times: user 5min 40s, sys: 26.3 s, total: 6min 7s  \n",
    "Wall time: 5min 46s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b95cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 大概40个epoch就可以了?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da3dbc0",
   "metadata": {},
   "source": [
    "## 用本地的图片进行测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92069dc4",
   "metadata": {},
   "source": [
    "上面我保存到mps 中执行的保存, 所以后面也需要使用mps才可以? 或者转换成onnx 统一的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f917c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {v['new_id']:v for i, v in json_content.items() if v.get(\"new_id\")!=None}\n",
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad8d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = VGG16_S(num_classes=20).to(device)\n",
    "# model.load_state_dict(torch.load(save_model_path))\n",
    "# model.load_state_dict(torch.load(\"20240125_13:41__vgg16_0.pth\"))\n",
    "model.load_state_dict(torch.load(\"20240126__vgg16_19_94.44444444444444.pth\"))  # 不用百分百的\n",
    "\n",
    "\n",
    "def eval_predict(model, image_path):\n",
    "#     test_img = os.path.join(\"cus_test_data\", test_img)\n",
    "    image = read_image(image_path, mode=torchvision.io.image.ImageReadMode.RGB)\n",
    "    # image = image.resize(64, 64)\n",
    "    plt.imshow(np.transpose(image, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    model.eval()\n",
    "    timg = transform(image)\n",
    "    timg = timg.to(device)\n",
    "    timg1 = timg.unsqueeze(0)\n",
    "    result = model(timg1)\n",
    "    result\n",
    "    _, predicted = torch.max(result, 1)\n",
    "    print(predicted.item())\n",
    "\n",
    "    print(predicted.item(), \n",
    "          new_dict[predicted.item()]['zh'], \n",
    "          new_dict[predicted.item()]['en'],\n",
    "          new_dict[predicted.item()]['desc'])\n",
    "    \n",
    "    return result, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85757df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 打开一个图片, 加一个预处理, 检查一下\n",
    "# import cv2 \n",
    "\n",
    "# test_img = \"cus_data/2.png\"\n",
    "# print(\"real label: \", test_img.split(\"/\")[-1])\n",
    "# # image = read_image()\n",
    "# # test_img = \"/Users/zhengyiming/Downloads/iShot_2024-01-24_16.17.44.png\"\n",
    "\n",
    "\n",
    "\n",
    "# for test_img in os.listdir(\"cus_test_data/\"):\n",
    "#     print(test_img)\n",
    "#     if str(test_img).split(\".\")[-1] not in [\"png\", \"jpeg\", \"jpg\"]:\n",
    "#         continue\n",
    "# #     test_img = \"cus_test_data/iShot_2024-01-25_14.09.48.png\"\n",
    "#     image_path = os.path.join(\"cus_test_data\", test_img)\n",
    "#     eval_predict(model, image_path)\n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958142dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"new_cus_data/1.bg_3.png\"\n",
    "image_path = \"/Users/zhengyiming/PycharmProjects/Isaac/new_cus_data/2.bg_8.png\"\n",
    "\n",
    "image_path = \"/Users/zhengyiming/Downloads/iShot_2024-01-26_12.22.24.png\"\n",
    "\n",
    "# image_path = \"/Users/zhengyiming/Downloads/iShot_2024-01-26_12.16.24.png\"\n",
    "# image_path = \"/Users/zhengyiming/Downloads/iShot_2024-01-26_12.16.01.png\"\n",
    "# image_path = \"/Users/zhengyiming/Downloads/iShot_2024-01-26_12.17.48.png\"\n",
    "# image_path = \"/Users/zhengyiming/Downloads/iShot_2024-01-26_12.16.20.png\"\n",
    "\n",
    "# for i in os.listdir(\"new_cus_data/\"):\n",
    "#     if i.split(\".\")[-1] not in [\"png\", \"jpeg\", \"jpg\"]:\n",
    "#         continue\n",
    "        \n",
    "# image_path = os.path.join(\"new_cus_data\", i)\n",
    "\n",
    "print(\"real label: \", image_path.split(\"/\")[-1])    \n",
    "# image_path = os.path.join(\"cus_test_data\", test_img)\n",
    "eval_predict(model, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56c838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e026ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "# probabilities_percent = probabilities * 100\n",
    "# for i in range(probabilities_percent.shape[0]):\n",
    "#     print(f\"Image {i+1}:\")\n",
    "#     for j in range(probabilities_percent.shape[1]):\n",
    "#         print(f\"    Class {j}: {probabilities_percent[i][j].item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img.split(\".\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454cb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes)\n",
    "\n",
    "## 明天把这个模型简单的封装一下, 增加一个前端, 摄像头框选的操作. 试试怎么搞\n",
    "# 封装起来直接可以用了, 有意思."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save my net model\n",
    "# print(epoch)\n",
    "# save_model_path = f\"net_{epoch}.pth\"\n",
    "# torch.save(net.state_dict(), save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00eed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecdbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = vgg16(num_classes=len(classes)).to(device)\n",
    "# model.load_state_dict(torch.load(save_model_path))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3844be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3772a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 检查模型, 说明模型根本没训练到\n",
    "# import torch\n",
    "# # 在测试集上进行推理验证\n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for data in train_dataloader:\n",
    "#         images, labels = data\n",
    "# #         images = images.float()\n",
    "\n",
    "#         ## 并排显示两个图片, 完全没训练到, 这是, loss\n",
    "#         print(len(labels))\n",
    "        \n",
    "#         sub_images = [images[i].numpy().transpose((1, 2, 0))\n",
    "#                      for i in range(batch_size)]\n",
    "        \n",
    "# #         img1 = images[0].numpy().transpose((1, 2, 0))\n",
    "# #         img2 = images[1].numpy().transpose((1, 2, 0))\n",
    "# #         combined_img = np.concatenate((img1, img2), axis=1)\n",
    "\n",
    "#         combined_img = np.concatenate(sub_images, axis=1)\n",
    "#         plt.imshow(combined_img)\n",
    "        \n",
    "#         images = images.float()\n",
    "\n",
    "        \n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "#         sub_labels = [new_dict[i]['en'] for i in predicted.tolist()]\n",
    "#         plt.title(sub_labels)\n",
    "#         plt.show()  # 归一化, 输入前. 这个倒也是没问题. \n",
    "\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "# accuracy = 100 * correct / total\n",
    "# print('Accuracy of the network on the test images: %d %%' % accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## vgg 16就足够了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 接下来的路线\n",
    "\n",
    "## 1. 增加训练数据量, 增加数据增强\n",
    "## 2. 测试和验证简单的 自定义模型\n",
    "\n",
    "## 3. 继续接下来的部署流程, \n",
    "### 3.1 部署到移动设备, 安卓或者 ios\n",
    "### 3.2 直接部署到安卓本身  \n",
    "\n",
    "## 4. 图像识别改成 目标检测!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de8034f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820b2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
